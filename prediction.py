# -*- coding: utf-8 -*-
"""prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQbnB_JVHQ-TjBGx4OTQaaVwwzG3tHOB
"""

#keras
import pandas as pd
import numpy
from tensorflow import keras
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

#nltk
import re
import nltk
import gc
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

import pickle
import statistics

nltk.download('stopwords')
TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
stop_words = stopwords.words("english")
stemmer = SnowballStemmer("english")


def preprocess(text, stem=False):
   # Remove link,user and special characters
    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            if stem:
                tokens.append(stemmer.stem(token))
            else:
                tokens.append(token)
    return " ".join(tokens)


def predict(text):
  tw = tokenizer.texts_to_sequences([text])
  tw = pad_sequences(tw, maxlen=300)
  prediction = (model.predict(tw).item())
  return prediction


path = 'C://Users//Tanmay//Desktop//Project//Oizys-Project//user-tweets.json'
df = pd.read_json(path, lines=True)

df.content = df.content.apply(lambda x: preprocess(x))

model = keras.models.load_model('C://Users//Tanmay//Desktop//Project//Oizys-Project//Model 3//Model3a')  # change this

with open('C://Users//Tanmay//Desktop//Project//Oizys-Project//Model 3//tokenizer3a.pickle', 'rb') as handle:  # change this
    tokenizer = pickle.load(handle)

score = []
df_list = df['content'].values.tolist()
for i in range(0, len(df_list)):
  text = df_list[i]
  x = predict(text)
  score.append(x)
print('The score is: ', round(statistics.mean(score), 5))
